{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Branchy-Alexnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg5Bg7k2d-Uq",
        "colab_type": "code",
        "outputId": "2821ac88-29cd-47bb-8dd2-1eb5048e5f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "# define pytorch device - useful for device-agnostic execution\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# define model parameters\n",
        "NUM_EPOCHS = 90\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_DIM = 32  # pixels\n",
        "NUM_CLASSES = 10  # 10 classes for Cifar-10 dataset\n",
        "DEVICE_IDS = [0]  # GPUs to use\n",
        "OUTPUT_DIR = 'alexnet_data_out'\n",
        "LOG_DIR = OUTPUT_DIR + '/tblogs'  # tensorboard logs\n",
        "CHECKPOINT_DIR = OUTPUT_DIR + '/models'  # model checkpoints\n",
        "\n",
        "# make checkpoint path directory\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "#Hack to debug in the middle of a sequential \n",
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(x.size())\n",
        "        return x\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural network model consisting of layers propsed by AlexNet paper.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        \"\"\"\n",
        "        Define and allocate layers for this neural net.\n",
        "        Args:\n",
        "            num_classes (int): number of classes to predict with this model\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        #main net\n",
        "        self.norm = nn.LocalResponseNorm(size=3, alpha=0.00005, beta=0.75)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, padding=2, stride=1)\n",
        "        self.conv2 = nn.Conv2d(64, 192, 5, padding=1)\n",
        "        self.conv3 = nn.Conv2d(192, 384, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(384, 256, 3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.mainConvLayers = [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5]\n",
        "\n",
        "        #Branch 1\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LocalResponseNorm(size=3, alpha=0.00005, beta=0.75),\n",
        "            nn.Conv2d(64, 32, 3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, 3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.branch1fc = nn.Linear(1568, 10)\n",
        "\n",
        "        #Branch 2\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.LocalResponseNorm(size=3, alpha=0.00005, beta=0.75),\n",
        "            nn.Conv2d(192, 32, 3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.branch2fc = nn.Linear(128, 10)\n",
        "\n",
        "        #linear layers of main branch\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=(256 * 2*2), out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5, inplace=True),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=4096, out_features=num_classes),\n",
        "        )\n",
        "        self.init_bias()  # initialize bias\n",
        "\n",
        "    def init_bias(self):\n",
        "        for layer in self.branch1:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        for layer in self.branch2:\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "                nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        for layer in self.mainConvLayers:\n",
        "            nn.init.normal_(layer.weight, mean=0, std=0.01)\n",
        "            nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "        # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.conv2.bias, 1)\n",
        "        nn.init.constant_(self.conv4.bias, 1)\n",
        "        nn.init.constant_(self.conv5.bias, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Pass the input through the net.\n",
        "        Args:\n",
        "            x (Tensor): input tensor\n",
        "        Returns:\n",
        "            output (Tensor): output tensor\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        #BRANCH 1: 2 3x3 conv and one FC layer\n",
        "        x1 = self.branch1(x)\n",
        "        x1 = x1.view(-1, 1568)\n",
        "        x1 = self.branch1fc(x1)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        #BRANCH 2: 1 3x3 conv and one FC layer\n",
        "        x2 = self.branch2(x)\n",
        "        x2 = x2.view(-1, 128)\n",
        "        x2 = self.branch2fc(x2)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.norm(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.conv5(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), 256 * 2 * 2)  # reduce the dimensions for linear layer input\n",
        "        x = self.classifier(x)\n",
        "        return x1, x2, x\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # print the seed value\n",
        "    seed = torch.initial_seed()\n",
        "    print('Used seed : {}'.format(seed))\n",
        "\n",
        "    tbwriter = SummaryWriter(log_dir=LOG_DIR)\n",
        "    print('TensorboardX summary writer created')\n",
        "\n",
        "    # create model\n",
        "    alexnet = AlexNet(num_classes=NUM_CLASSES).to(device)\n",
        "    # train on multiple GPUs\n",
        "    alexnet = torch.nn.parallel.DataParallel(alexnet, device_ids=DEVICE_IDS)\n",
        "    print(alexnet)\n",
        "    print('AlexNet created')\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            #transforms.RandomRotation(15),\n",
        "            transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "            transforms.CenterCrop(IMAGE_DIM),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "    \n",
        "    dataset = datasets.CIFAR10(root='./data', train=True, transform=transform,\n",
        "                               download=True)\n",
        "    \n",
        "    testtransform = transforms.Compose([\n",
        "        transforms.CenterCrop(IMAGE_DIM),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "    testdata = datasets.CIFAR10(root='./data', train=False, transform=testtransform, download=True)\n",
        "    print('Dataset created')\n",
        "    \n",
        "    dataloader = data.DataLoader(\n",
        "        dataset,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=8,\n",
        "        drop_last=True,\n",
        "        batch_size=BATCH_SIZE)\n",
        "    print('Dataloader created')\n",
        "\n",
        "    optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n",
        "    print('Optimizer created')\n",
        "\n",
        "    # multiply LR by 1 / 10 after every 30 epochs\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    print('LR Scheduler created')\n",
        "\n",
        "    # start training!!\n",
        "    print('Starting training...')\n",
        "    alexnet.train()\n",
        "    total_steps = 1\n",
        "    end = False\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        for imgs, classes in dataloader:\n",
        "            imgs, classes = imgs.to(device), classes.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # calculate the loss\n",
        "            output = alexnet(imgs)[-1]\n",
        "            loss = F.cross_entropy(output, classes)\n",
        "\n",
        "            # update the parameters\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # log the information and add to tensorboard\n",
        "            if total_steps % 10 == 0:\n",
        "                #with torch.no_grad():\n",
        "                _, preds = torch.max(output, 1)\n",
        "                accuracy = torch.sum(preds == classes)\n",
        "\n",
        "                print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
        "                    .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
        "\n",
        "            if total_steps % 300 == 0:\n",
        "\n",
        "                #~~~~~~~VALIDATION~~~~~~~~~\n",
        "                valdataloader = data.DataLoader(\n",
        "                    testdata,\n",
        "                    shuffle=True,\n",
        "                    pin_memory=True,\n",
        "                    num_workers=8,\n",
        "                    drop_last=True,\n",
        "                    batch_size=128)\n",
        "                correct_count = 0\n",
        "                total_count = 0\n",
        "                alexnet.eval()\n",
        "                for images, labels in valdataloader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    with torch.no_grad(): #no gradient descent!\n",
        "                        logps = alexnet(images)[-1]\n",
        "                    \n",
        "                    for i in range(BATCH_SIZE):\n",
        "                        ps = torch.exp(logps)\n",
        "                        prob = list(ps.cpu().numpy()[i])\n",
        "                        pred_label = prob.index(max(prob))\n",
        "                        true_label = labels.cpu().numpy()[i]\n",
        "                        if(true_label == pred_label):\n",
        "                            correct_count += 1\n",
        "                        total_count += 1\n",
        "                print(\"Number Of Images Tested =\", total_count)\n",
        "                print(\"\\nModel Accuracy =\", (correct_count/total_count))\n",
        "                if correct_count/total_count > 0.83:\n",
        "                    end = True\n",
        "                alexnet.train()\n",
        "            if end:\n",
        "                break\n",
        "\n",
        "            total_steps += 1\n",
        "        if end:\n",
        "            break\n",
        "        lr_scheduler.step()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.17.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (41.6.0)\n",
            "Used seed : 13681243636625788801\n",
            "TensorboardX summary writer created\n",
            "DataParallel(\n",
            "  (module): AlexNet(\n",
            "    (norm): LocalResponseNorm(3, alpha=5e-05, beta=0.75, k=1.0)\n",
            "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (conv2): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
            "    (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv4): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (relu): ReLU()\n",
            "    (branch1): Sequential(\n",
            "      (0): ReLU()\n",
            "      (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (2): LocalResponseNorm(3, alpha=5e-05, beta=0.75, k=1.0)\n",
            "      (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU()\n",
            "      (5): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (6): ReLU()\n",
            "      (7): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (branch1fc): Linear(in_features=1568, out_features=10, bias=True)\n",
            "    (branch2): Sequential(\n",
            "      (0): ReLU()\n",
            "      (1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "      (2): LocalResponseNorm(3, alpha=5e-05, beta=0.75, k=1.0)\n",
            "      (3): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (4): ReLU()\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (branch2fc): Linear(in_features=128, out_features=10, bias=True)\n",
            "    (classifier): Sequential(\n",
            "      (0): Dropout(p=0.5, inplace=True)\n",
            "      (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (2): ReLU()\n",
            "      (3): Dropout(p=0.5, inplace=True)\n",
            "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "      (5): ReLU()\n",
            "      (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "AlexNet created\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Dataset created\n",
            "Dataloader created\n",
            "Optimizer created\n",
            "LR Scheduler created\n",
            "Starting training...\n",
            "Epoch: 1 \tStep: 10 \tLoss: 2.3151 \tAcc: 11\n",
            "Epoch: 1 \tStep: 20 \tLoss: 2.2882 \tAcc: 18\n",
            "Epoch: 1 \tStep: 30 \tLoss: 2.3209 \tAcc: 16\n",
            "Epoch: 1 \tStep: 40 \tLoss: 2.3291 \tAcc: 9\n",
            "Epoch: 1 \tStep: 50 \tLoss: 2.3034 \tAcc: 16\n",
            "Epoch: 1 \tStep: 60 \tLoss: 2.3313 \tAcc: 12\n",
            "Epoch: 1 \tStep: 70 \tLoss: 2.3093 \tAcc: 9\n",
            "Epoch: 1 \tStep: 80 \tLoss: 2.2999 \tAcc: 17\n",
            "Epoch: 1 \tStep: 90 \tLoss: 2.3047 \tAcc: 16\n",
            "Epoch: 1 \tStep: 100 \tLoss: 2.2774 \tAcc: 19\n",
            "Epoch: 1 \tStep: 110 \tLoss: 2.2327 \tAcc: 14\n",
            "Epoch: 1 \tStep: 120 \tLoss: 2.2236 \tAcc: 20\n",
            "Epoch: 1 \tStep: 130 \tLoss: 2.1853 \tAcc: 24\n",
            "Epoch: 1 \tStep: 140 \tLoss: 2.0961 \tAcc: 21\n",
            "Epoch: 1 \tStep: 150 \tLoss: 2.0151 \tAcc: 22\n",
            "Epoch: 1 \tStep: 160 \tLoss: 2.0284 \tAcc: 27\n",
            "Epoch: 1 \tStep: 170 \tLoss: 1.9976 \tAcc: 33\n",
            "Epoch: 1 \tStep: 180 \tLoss: 2.0285 \tAcc: 24\n",
            "Epoch: 1 \tStep: 190 \tLoss: 1.9910 \tAcc: 24\n",
            "Epoch: 1 \tStep: 200 \tLoss: 2.1028 \tAcc: 31\n",
            "Epoch: 1 \tStep: 210 \tLoss: 2.0186 \tAcc: 29\n",
            "Epoch: 1 \tStep: 220 \tLoss: 2.0065 \tAcc: 32\n",
            "Epoch: 1 \tStep: 230 \tLoss: 1.9799 \tAcc: 30\n",
            "Epoch: 1 \tStep: 240 \tLoss: 1.9884 \tAcc: 34\n",
            "Epoch: 1 \tStep: 250 \tLoss: 2.0165 \tAcc: 26\n",
            "Epoch: 1 \tStep: 260 \tLoss: 1.8281 \tAcc: 46\n",
            "Epoch: 1 \tStep: 270 \tLoss: 1.9058 \tAcc: 35\n",
            "Epoch: 1 \tStep: 280 \tLoss: 1.9858 \tAcc: 30\n",
            "Epoch: 1 \tStep: 290 \tLoss: 1.9452 \tAcc: 32\n",
            "Epoch: 1 \tStep: 300 \tLoss: 2.0057 \tAcc: 32\n",
            "Number Of Images Tested = 9984\n",
            "\n",
            "Model Accuracy = 0.3078926282051282\n",
            "Epoch: 1 \tStep: 310 \tLoss: 1.8556 \tAcc: 44\n",
            "...............................................\n",
            "Epoch: 58 \tStep: 22320 \tLoss: 0.2046 \tAcc: 117\n",
            "Epoch: 58 \tStep: 22330 \tLoss: 0.1412 \tAcc: 123\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zORtNqJkyntP",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsV69JIDypVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingData = datasets.CIFAR10(root='./data', train=False, transform=testtransform,\n",
        "                               download=True)\n",
        "dataloader = data.DataLoader(\n",
        "    testingData,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "alexnet.eval()\n",
        "for imgs, classes in dataloader:\n",
        "    imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "    with torch.no_grad(): #no gradient descent!\n",
        "        logps = alexnet(imgs)[-1]\n",
        "    for i in range(BATCH_SIZE):\n",
        "        ps = torch.exp(logps)\n",
        "        prob = list(ps.cpu().numpy()[i])\n",
        "        pred_labels = prob.index(max(prob))\n",
        "        true_labels = classes.cpu().numpy()[i]\n",
        "        if(true_labels == pred_labels):\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "    if total_count % 10 == 0:\n",
        "        print(f\"total: {total_count}, correct: {correct_count}\")\n",
        "print(\"Number Of Images Tested =\", total_count)\n",
        "print(\"\\nModel Accuracy =\", (correct_count/total_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdATIh-FXjIg",
        "colab_type": "text"
      },
      "source": [
        "# Look at the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGZcPQrHXlMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    # print and save the grad of the parameters\n",
        "    # also print and save parameter values\n",
        "    print('*' * 10)\n",
        "    for name, parameter in alexnet.named_parameters():\n",
        "        if parameter.grad is not None:\n",
        "            avg_grad = torch.mean(parameter.grad)\n",
        "            print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
        "            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
        "            tbwriter.add_histogram('grad/{}'.format(name),\n",
        "                    parameter.grad.cpu().numpy(), total_steps)\n",
        "        if parameter.data is not None:\n",
        "            avg_weight = torch.mean(parameter.data)\n",
        "            print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
        "            tbwriter.add_histogram('weight/{}'.format(name),\n",
        "                    parameter.data.cpu().numpy(), total_steps)\n",
        "            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tN-bR8RZ_Wp",
        "colab_type": "text"
      },
      "source": [
        "# Testing to confirm branches did NOT learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6uz3ghEaEkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingData = datasets.CIFAR10(root='./data', train=False, transform=testtransform,\n",
        "                               download=True)\n",
        "dataloader = data.DataLoader(\n",
        "    testingData,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "alexnet.eval()\n",
        "for imgs, classes in dataloader:\n",
        "    imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "    with torch.no_grad(): #no gradient descent!\n",
        "        logps = alexnet(imgs)[0]\n",
        "    for i in range(BATCH_SIZE):\n",
        "        ps = torch.exp(logps)\n",
        "        prob = list(ps.cpu().numpy()[i])\n",
        "        pred_labels = prob.index(max(prob))\n",
        "        true_labels = classes.cpu().numpy()[i]\n",
        "        if(true_labels == pred_labels):\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "print(\"\\nBranch1 Accuracy =\", (correct_count/total_count))\n",
        "\n",
        "testingData = datasets.CIFAR10(root='./data', train=False, transform=testtransform,\n",
        "                               download=True)\n",
        "dataloader = data.DataLoader(\n",
        "    testingData,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "correct_count = 0\n",
        "total_count = 0\n",
        "alexnet.eval()\n",
        "for imgs, classes in dataloader:\n",
        "    imgs, classes = imgs.to(device), classes.to(device)\n",
        "\n",
        "    with torch.no_grad(): #no gradient descent!\n",
        "        logps = alexnet(imgs)[1]\n",
        "    for i in range(BATCH_SIZE):\n",
        "        ps = torch.exp(logps)\n",
        "        prob = list(ps.cpu().numpy()[i])\n",
        "        pred_labels = prob.index(max(prob))\n",
        "        true_labels = classes.cpu().numpy()[i]\n",
        "        if(true_labels == pred_labels):\n",
        "            correct_count += 1\n",
        "        total_count += 1\n",
        "\n",
        "print(\"\\nBranch2 Accuracy =\", (correct_count/total_count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktKm3zZgcNT_",
        "colab_type": "text"
      },
      "source": [
        "# Train now with branches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYqFOp79cQ1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_EPOCHS = 70\n",
        "\n",
        "transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #transforms.RandomRotation(15),\n",
        "        transforms.RandomResizedCrop(IMAGE_DIM, scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n",
        "        transforms.CenterCrop(IMAGE_DIM),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, transform=transform,\n",
        "                            download=True)\n",
        "\n",
        "testtransform = transforms.Compose([\n",
        "    transforms.CenterCrop(IMAGE_DIM),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
        "testdata = datasets.CIFAR10(root='./data', train=False, transform=testtransform, download=True)\n",
        "print('Dataset created')\n",
        "\n",
        "dataloader = data.DataLoader(\n",
        "    dataset,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "print('Dataloader created')\n",
        "\n",
        "optimizer = optim.Adam(params=alexnet.parameters(), lr=0.0001)\n",
        "print('Optimizer created')\n",
        "\n",
        "# multiply LR by 1 / 10 after every 30 epochs\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "print('LR Scheduler created')\n",
        "\n",
        "# start training!!\n",
        "print('Starting training...')\n",
        "alexnet.train()\n",
        "total_steps = 1\n",
        "end = False\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for imgs, classes in dataloader:\n",
        "        imgs, classes = imgs.to(device), classes.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # calculate the loss\n",
        "        outputb1, outputb2, outputmain = alexnet(imgs)\n",
        "        lossb1 = F.cross_entropy(outputb1, classes)\n",
        "        lossb2 = F.cross_entropy(outputb2, classes)\n",
        "        lossmain = F.cross_entropy(outputmain, classes)\n",
        "        loss = lossb1 + 0.6*lossb2 + 0.3*lossmain\n",
        "\n",
        "        # update the parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # log the information and add to tensorboard\n",
        "        if total_steps % 10 == 0:\n",
        "            #with torch.no_grad():\n",
        "            _, preds = torch.max(outputmain, 1)\n",
        "            accuracy = torch.sum(preds == classes)\n",
        "\n",
        "            print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
        "                .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
        "            print(f'Branch 1: loss {lossb1}')\n",
        "            print(f'Branch 2: loss {lossb2}')\n",
        "            print(f'Main: loss {lossmain}')\n",
        "\n",
        "        if total_steps % 300 == 0:\n",
        "\n",
        "            #~~~~~~~VALIDATION~~~~~~~~~\n",
        "            valdataloader = data.DataLoader(\n",
        "                testdata,\n",
        "                shuffle=True,\n",
        "                pin_memory=True,\n",
        "                num_workers=8,\n",
        "                drop_last=True,\n",
        "                batch_size=128)\n",
        "            b1_correct = 0\n",
        "            b2_correct = 0\n",
        "            main_correct = 0\n",
        "            total_count = 0\n",
        "            alexnet.eval()\n",
        "            for image, label in valdataloader:\n",
        "                image, label = image.to(device), label.to(device)\n",
        "                with torch.no_grad(): #no gradient descent!\n",
        "                    b1, b2, main = alexnet(image)\n",
        "                \n",
        "                b1ps = torch.exp(b1)\n",
        "                b2ps = torch.exp(b2)\n",
        "                mainps = torch.exp(main)\n",
        "                for i in range(BATCH_SIZE):\n",
        "                    b1prob = list(b1ps.cpu().numpy()[i])\n",
        "                    b2prob = list(b2ps.cpu().numpy()[i])\n",
        "                    mainprob = list(main.cpu().numpy()[i])\n",
        "\n",
        "\n",
        "                    b1label = b1prob.index(max(b1prob))\n",
        "                    b2label = b2prob.index(max(b2prob))\n",
        "                    mainlabel = mainprob.index(max(mainprob))\n",
        "\n",
        "                    true_label = label.cpu().numpy()[i]\n",
        "\n",
        "                    if(true_label == b1label):\n",
        "                        b1_correct += 1\n",
        "\n",
        "                    if(true_label == b2label):\n",
        "                        b2_correct += 1\n",
        "\n",
        "                    if(true_label == mainlabel):\n",
        "                        main_correct += 1\n",
        "\n",
        "                    total_count += 1\n",
        "                    if(total_count > 1500):\n",
        "                        break\n",
        "            print(\"Number Of Images Tested =\", total_count*128)\n",
        "            print(f\"b1 acc: {b1_correct/total_count}, b2 acc: {b2_correct/total_count}, main acc: {main_correct/total_count}\")\n",
        "            alexnet.train()\n",
        "        if end:\n",
        "            break\n",
        "\n",
        "        total_steps += 1\n",
        "    if end:\n",
        "        break\n",
        "    lr_scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwR8Iug3M5b",
        "colab_type": "text"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MumK1GJm3QS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cKXfgrK5Lqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        # save checkpoints\n",
        "        checkpoint_path = os.path.join(\"/content/drive/My Drive/Colab Notebooks/models\", 'Branchy-alexnet_states.pkl'.format(epoch + 1))\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'total_steps': total_steps,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': alexnet.state_dict(),\n",
        "            'seed': seed,\n",
        "        }\n",
        "        torch.save(state, checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2hEzJzS0jY6",
        "colab_type": "text"
      },
      "source": [
        "# Testing all branches "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC0Tmhay4x0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "\n",
        "testingData = datasets.CIFAR10(root='./data', train=False, transform=testtransform,\n",
        "                               download=True)\n",
        "dataloader = data.DataLoader(\n",
        "    testingData,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=8,\n",
        "    drop_last=True,\n",
        "    batch_size=BATCH_SIZE)\n",
        "\n",
        "b1_correct = 0\n",
        "b2_correct = 0\n",
        "main_correct = 0\n",
        "total_count = 0\n",
        "total_correct = 0\n",
        "b1exit = 0\n",
        "b2exit = 0\n",
        "mainexit = 0\n",
        "alexnet.eval()\n",
        "for image, label in dataloader:\n",
        "    image, label = image.to(device), label.to(device)\n",
        "    with torch.no_grad(): #no gradients\n",
        "        b1, b2, main = alexnet(image)\n",
        "    \n",
        "    b1ps = torch.exp(b1)\n",
        "    b2ps = torch.exp(b2)\n",
        "    mainps = torch.exp(main)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        b1prob = list(b1ps.cpu().numpy()[i])\n",
        "        b2prob = list(b2ps.cpu().numpy()[i])\n",
        "        mainprob = list(main.cpu().numpy()[i])\n",
        "\n",
        "        true_label = label.cpu().numpy()[i]\n",
        "        \n",
        "        labelvector = [0]*10\n",
        "        labelvector[true_label-1] = 1\n",
        "        print(torch.Tensor(labelvector).size())\n",
        "        b1loss = F.binary_cross_entropy(b1, torch.Tensor(labelvector).cuda())\n",
        "        b2loss = F.binary_cross_entropy(b2, torch.Tensor(labelvector).cuda())\n",
        "        mainloss = F.binary_cross_entropy(main, torch.Tensor(labelvector).cuda())\n",
        "\n",
        "        b1label = b1prob.index(max(b1prob))\n",
        "        b2label = b2prob.index(max(b2prob))\n",
        "        mainlabel = mainprob.index(max(mainprob))\n",
        "\n",
        "\n",
        "        if(true_label == b1label):\n",
        "            b1_correct += 1\n",
        "\n",
        "        if(true_label == b2label):\n",
        "            b2_correct += 1\n",
        "\n",
        "        if(true_label == mainlabel):\n",
        "            main_correct += 1\n",
        "\n",
        "        if b1loss < 0.1:\n",
        "            b1exit +=1\n",
        "            if(true_label == b1label):\n",
        "                total_correct += 1\n",
        "        elif b2loss < 0.5:\n",
        "            b2exit +=1\n",
        "            if(true_label == b2label):\n",
        "                total_correct += 1\n",
        "        else:\n",
        "            mainexit +=1\n",
        "            if(true_label == mainlabel):\n",
        "                total_correct += 1\n",
        "\n",
        "        total_count += 1\n",
        "print(\"Number Of Images Tested =\", total_count)\n",
        "print(\"Overall network accuracy with exits at branches: \", correct_count/total_count)\n",
        "print(f\"b1 acc: {b1_correct/total_count}, b2 acc: {b2_correct/total_count}, main acc: {main_correct/total_count}\")\n",
        "print(f'b1 exit amt: {b1exit}, b2 exit amt: {b2exit}, main exit amt: {mainexit}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
